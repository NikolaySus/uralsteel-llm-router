r"""
gRPC —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é.
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ llm_pb2.py –∏ llm_pb2_grpc.py –∏–∑ llm.proto:
uv run -m grpc_tools.protoc -I.\uralsteel-grpc-api\llm\ --python_out=.
--grpc_python_out=. llm.proto
"""

from datetime import datetime
from concurrent import futures
from io import BytesIO
import logging
import os

from google.protobuf import empty_pb2
import grpc
from grpc import aio
from openai import OpenAI

import llm_pb2
import llm_pb2_grpc


# –°–ª–æ–≤–∏–ª –∫—Ä–∏–Ω–∂ —Å systemd...
# –§–∏–∫—Å –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ –¥—Ä—É–≥–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
for key, value in os.environ.items():
    if "${" in value:
        os.environ[key] = os.path.expandvars(value)


MODEL = os.getenv('MODEL', "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")
BASE_URL = os.getenv('BASE_URL', "http://127.0.0.1:8008/v1")
API_KEY = os.getenv('API_KEY', "uralsteel")
CLOUD_FOLDER = os.getenv('CLOUD_FOLDER', "uralsteel")
SPEECH2TEXT_OPEN_AI = os.environ.get('SPEECH2TEXT_OPEN_AI', '')
SPEECH2TEXT_MODEL = os.environ.get('SPEECH2TEXT_MODEL', '')
BASE_URL_OPEN_AI = os.environ.get('BASE_URL_OPEN_AI', '')
SECRET_KEY = os.environ.get('SECRET_KEY', '')


# ============================================================================
# –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø - Interceptor –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–∞
# ============================================================================

class AuthInterceptor(grpc.ServerInterceptor):
    """
    gRPC Interceptor –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞.
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Interceptor –≤–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤ –º–µ—Ç–æ–¥–∞—Ö:
    1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –î–û –≤—ã–∑–æ–≤–∞ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏ (–º–∏–Ω–∏–º—É–º –∑–∞—Ç—Ä–∞—Ç CPU)
    2. –ù–µ–∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –æ—Ç–∫–ª–æ–Ω—è—é—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–µ—Ç–∏
    3. –ù–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ –≤ –∫–∞–∂–¥–æ–º –º–µ—Ç–æ–¥–µ
    4. –õ–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –ø—É–±–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    
    –ú–µ—Ç–æ–¥—ã –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (public):
    - Ping: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è health-check'–æ–≤, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
    
    –ú–µ—Ç–æ–¥—ã —Ç—Ä–µ–±—É—é—â–∏–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (protected):
    - NewMessage: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM, –∫—Ä–∏—Ç–∏—á–Ω—ã–π —Ä–µ—Å—É—Ä—Å
    - AvailableModelsText2Text: –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö
    - AvailableModelsSpeech2Text: –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö
    """
    
    # –ú–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —Ç—Ä–µ–±—É—é—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é (public)
    PUBLIC_METHODS = {
        '/llm.Llm/Ping',
    }
    
    def intercept_service(self, continuation, handler_call_details):
        """
        –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ RPC –º–µ—Ç–æ–¥–∞.
        
        handler_call_details.invocation_metadata —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞,
        –≤–∫–ª—é—á–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
        
        –ú–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ª–∏–±–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, –ª–∏–±–æ –≤—ã–∑–≤–∞—Ç—å continuation()
        –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–∞–ª—å—à–µ.
        """
        method_name = handler_call_details.method
        
        # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ –≤ —Å–ø–∏—Å–∫–µ –ø—É–±–ª–∏—á–Ω—ã—Ö - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        if method_name in self.PUBLIC_METHODS:
            return continuation(handler_call_details)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata = dict(handler_call_details.invocation_metadata or [])
        authorization = metadata.get('authorization', '')
        
        # –û–∂–∏–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç "Bearer <SECRET_KEY>" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á
        secret_from_header = authorization.replace('Bearer ', '').strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞
        if not SECRET_KEY:
            # –ï—Å–ª–∏ SECRET_KEY –Ω–µ –∑–∞–¥–∞–Ω –≤ env - –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
            # –Ω–æ –ù–ï –æ—Ç–∫–ª–æ–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å dev —Å—Ä–µ–¥–æ–π)
            print(f"‚ö† WARNING: SECRET_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. "
                  f"–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ –¥–ª—è –º–µ—Ç–æ–¥–∞ {method_name}")
            return continuation(handler_call_details)
        
        if secret_from_header != SECRET_KEY:
            # –°–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á –Ω–µ–≤–µ—Ä–µ–Ω - –æ—Ç–∫–ª–æ–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            print(f"‚ùå UNAUTHORIZED: –ù–µ–≤–µ—Ä–Ω—ã–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π SECRET_KEY –¥–ª—è –º–µ—Ç–æ–¥–∞ {method_name}")
            # –°–æ–∑–¥–∞—ë–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–∫–∏
            abort_handler = grpc.unary_unary_rpc_method_handler(
                lambda request, context: context.abort(
                    grpc.StatusCode.UNAUTHENTICATED,
                    "Invalid or missing authorization"
                )
            )
            return abort_handler(None, handler_call_details)
        
        # –°–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á –≤–µ—Ä–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–∞–ª—å—à–µ
        return continuation(handler_call_details)


def available_models(base_url: str, api_key: str, project: str):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫."""
    models_list = OpenAI(
        base_url=base_url,
        api_key=api_key,
        project=project,
    ).models.list()
    check_arr = []
    for model in models_list.data:
        check_arr.append(model.id)
    return check_arr


def transcribe_audio_buffer(audio_buffer: BytesIO,
                            speech2text_override: str = None):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–π audio_buffer –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂
    (transcription_text, TranscribeResponseType proto).

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç speech2text_override –µ—Å–ª–∏ –∑–∞–¥–∞–Ω, –∏–Ω–∞—á–µ SPEECH2TEXT_MODEL.
    –ë—Ä–æ—Å–∞–µ—Ç ValueError, –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è SPEECH2TEXT –Ω–µ –∑–∞–¥–∞–Ω–∞.
    """
    if audio_buffer.tell() == 0:
        return None, None

    if not SPEECH2TEXT_OPEN_AI or (
        not SPEECH2TEXT_MODEL or not BASE_URL_OPEN_AI):
        raise ValueError(
            "Speech-to-text —Å–µ—Ä–≤–∏—Å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: "
            "SPEECH2TEXT_OPEN_AI, SPEECH2TEXT_MODEL, BASE_URL_OPEN_AI"
        )

    audio_buffer.seek(0)
    # –ù–µ–∫–æ—Ç–æ—Ä—ã–º API —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–º—è —É —Ñ–∞–π–ª–∞
    audio_buffer.name = "audio.mp3"

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –±–µ—Ä—ë–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if speech2text_override:
        model_to_use = speech2text_override
    else:
        model_to_use = SPEECH2TEXT_MODEL

    transcription = OpenAI(
        base_url=BASE_URL_OPEN_AI,
        api_key=SPEECH2TEXT_OPEN_AI,
    ).audio.transcriptions.create(
        model=model_to_use,
        file=audio_buffer
    )
    text = getattr(transcription, "text", "")
    usage = getattr(transcription, "usage", None)
    duration = None
    if usage:
        duration = getattr(usage, "duration", None)
    proto = llm_pb2.TranscribeResponseType(
        transcription=text,
        duration=duration,
        datetime=datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
    )
    return text, proto


def build_messages_from_history(history, user_message: str,
                                text2text_override: str = None):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è LLM –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ history –∏ user_message.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç text2text_override –µ—Å–ª–∏ –∑–∞–¥–∞–Ω, –∏–Ω–∞—á–µ MODEL.
    """
    if text2text_override:
        model_to_use = text2text_override
    else:
        model_to_use = MODEL
    messages = [
        {
            "role": "system",
            "content": (
                "You are helpfull and highly skilled LLM-powered "
                "assistant that always follows best practices. "
                f"The base LLM is {model_to_use}. Current date and time: "
                f"{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}. "
                "Note that current date and time are relevant only "
                "for last message, previous ones could be sent a long "
                "time ago. Respond in the same language as the user."
            )
        }
    ]
    if history:
        for message in history:
            messages.append({
                "role": llm_pb2.Role.Name(message.role),
                "content": message.body
            })
    messages.append({"role": "user", "content": user_message})
    return messages


def responses_from_llm_chunk(chunk):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Ç–æ–∫–∞ –æ—Ç–≤–µ—Ç–∞ LLM –≤ –æ–¥–∏–Ω
    —ç–∫–∑–µ–º–ø–ª—è—Ä llm_pb2.NewMessageResponse –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    # delta content/reasoning_content, –µ—Å–ª–∏ –µ—Å—Ç—å
    delta_content = None
    delta_reasoning_content = None
    finish_reason = None
    if hasattr(chunk, "choices") and chunk.choices:
        choice0 = chunk.choices[0]
        delta = getattr(choice0, "delta", None)
        finish_reason = getattr(choice0, "finish_reason", None)
        if delta is not None:
            if getattr(delta, "content", None) is not None:
                delta_content = delta.content
            if getattr(delta, "reasoning_content", None) is not None:
                delta_reasoning_content = delta.reasoning_content

    # usage –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –µ—Å–ª–∏ –µ—Å—Ç—å
    completion_tokens = None
    prompt_tokens = None
    total_tokens = None
    if hasattr(chunk, "usage") and chunk.usage:
        usage = chunk.usage
        completion_tokens = getattr(usage, "completion_tokens", None)
        prompt_tokens = getattr(usage, "prompt_tokens", None)
        total_tokens = getattr(usage, "total_tokens", None)

    # –ï—Å–ª–∏ –µ—Å—Ç—å usage –∏–ª–∏ –∫–æ–Ω–µ—á–Ω—ã–π —Å–∏–≥–Ω–∞–ª, —Å–æ–∑–¥–∞—ë–º CompleteResponseType
    if (finish_reason == "stop") or (
        total_tokens is not None and (
            prompt_tokens is not None and completion_tokens is not None)):
        return llm_pb2.NewMessageResponse(
            complete=llm_pb2.CompleteResponseType(
                prompt_tokens=(prompt_tokens or 0),
                completion_tokens=(completion_tokens or 0),
                total_tokens=(total_tokens or 0),
                datetime=datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
            )
        )
    # –ï—Å–ª–∏ –æ–¥–∏–Ω –∏–∑ delta content –µ—Å—Ç—å, —Å–æ–∑–¥–∞—ë–º GenerateResponseType
    elif delta_content is not None or delta_reasoning_content is not None:
        return llm_pb2.NewMessageResponse(
            generate=llm_pb2.GenerateResponseType(
                content=(delta_content or ""),
                reasoning_content=(delta_reasoning_content or ""),
                datetime=datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
            )
        )
    else:
        # –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —á–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ ‚Äî –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        print(f"WARN: {chunk}")
        return None


class LlmServicer(llm_pb2_grpc.LlmServicer):
    """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å–µ—Ä–≤–∏—Å–∞."""

    def Ping(self, request, context):
        """–ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞."""
        return empty_pb2.Empty()

    def AvailableModelsText2Text(self, request, context):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Text2Text –º–æ–¥–µ–ª–µ–π."""
        try:
            return llm_pb2.ModelsListResponse(
                models=available_models(BASE_URL, API_KEY, CLOUD_FOLDER))
        except Exception as e:
            print(f"ERROR getting text2text models: {e}")
            context.set_details(f"ERROR getting text2text models: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            return llm_pb2.ModelsListResponse(models=[])

    def AvailableModelsSpeech2Text(self, request, context):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Speech2Text –º–æ–¥–µ–ª–µ–π."""
        try:
            return llm_pb2.ModelsListResponse(
                models=available_models(BASE_URL_OPEN_AI,
                                        SPEECH2TEXT_OPEN_AI, None))
        except Exception as e:
            print(f"ERROR getting speech2text models: {e}")
            context.set_details(f"ERROR getting speech2text models: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            return llm_pb2.ModelsListResponse(models=[])

    def NewMessage(self, request_iter, context):
        """–ú–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞.
        
        request_iter ‚Äî –ø–æ—Ç–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (stream), –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        - –û–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å msg (—Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ)
        - –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π —Å mp3_chunk (–ø–æ—Ç–æ–∫–æ–≤–æ–µ –∞—É–¥–∏–æ)
        
        –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –≤ –æ–¥–Ω–æ–º –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è
        text2text_model –∏ speech2text_model –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        """
        try:
            user_message = None
            history = None
            audio_buffer = BytesIO()
            text2text_override = None
            speech2text_override = None

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Ç–æ–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
            for request in request_iter:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –Ω–µ–ø—É—Å—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                if history is None and request.history:
                    history = request.history

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                if text2text_override is None and request.text2text_model:
                    text2text_override = request.text2text_model
                if speech2text_override is None and request.speech2text_model:
                    speech2text_override = request.speech2text_model

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–æ–π —Ç–∏–ø payload –ø—Ä–∏—à–µ–ª
                if request.HasField("msg"):
                    # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                    user_message = request.msg
                elif request.HasField("mp3_chunk"):
                    # –°–æ–±–∏—Ä–∞–µ–º –∞—É–¥–∏–æ—á–∞–Ω–∫
                    audio_buffer.write(request.mp3_chunk)

            # –ï—Å–ª–∏ –ø—Ä–∏—à–ª–æ –∞—É–¥–∏–æ, —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –µ–≥–æ
            if audio_buffer.tell() > 0:
                user_message, transcribe_proto = transcribe_audio_buffer(
                    audio_buffer,
                    speech2text_override
                )
                if transcribe_proto is not None:
                    yield llm_pb2.NewMessageResponse(
                        transcribe=transcribe_proto)

            # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—É—Å—Ç–æ, –æ—à–∏–±–∫–∞
            if not user_message:
                raise ValueError("–ù–∏ —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è, –Ω–∏ –∞—É–¥–∏–æ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã")

            # –ò—Å—Ç–æ—Ä–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—É—Å—Ç–∞
            if history is None:
                history = []

            # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞
            messages = build_messages_from_history(history, user_message,
                                                   text2text_override)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            if text2text_override:
                model_to_use = text2text_override
            else:
                model_to_use = MODEL

            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI API –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞, –µ—Å–ª–∏
            # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –æ—Ç–º–µ–Ω—è–ª –∑–∞–ø—Ä–æ—Å
            if context.is_active():
                try:
                    response = OpenAI(
                        base_url=BASE_URL,
                        api_key=API_KEY,
                        project=CLOUD_FOLDER,
                    ).chat.completions.create(
                        model=model_to_use,
                        messages=messages,
                        max_tokens=2000,
                        temperature=0.3,
                        stream=True
                    )
                    for chunk in response:
                        if context.is_active() is False:
                            print("Client cancelled, stopping stream.")
                            break
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º chunk –≤ –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç protobuf (–∏–ª–∏ None)
                        resp = responses_from_llm_chunk(chunk)
                        if resp is not None:
                            yield resp
                except Exception as e:
                    print(f"STREAM ERROR: {e}")
                    context.set_code(grpc.StatusCode.INTERNAL)
                    context.set_details(f"STREAM ERROR: {e}")
                finally:
                    response.response.close()
            else:
                print("Client cancelled, stopping stream.")
        except Exception as e:
            print(f"ERROR: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"ERROR: {e}")


def serve():
    """–ó–∞–ø—É—Å–∫ gRPC —Å–µ—Ä–≤–µ—Ä–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π."""
    with open("server.crt", "rb") as f:
        server_cert = f.read()
    with open("server.key", "rb") as f:
        server_key = f.read()
    server_creds = grpc.ssl_server_credentials(
        [(server_key, server_cert)]
    )
    
    # –°–æ–∑–¥–∞—ë–º —Å–µ—Ä–≤–µ—Ä —Å Interceptor –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    # Interceptor –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ RPC –≤—ã–∑–æ–≤–∞
    # –ü–ï–†–ï–î —Ç–µ–º –∫–∞–∫ –≤—ã–∑–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É –º–µ—Ç–æ–¥–∞
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=10),
        interceptors=[AuthInterceptor()]  # –î–æ–±–∞–≤–ª—è–µ–º Interceptor
    )
    
    llm_pb2_grpc.add_LlmServicer_to_server(
        LlmServicer(), server
    )
    server.add_secure_port("[::]:50051", server_creds)
    server.start()
    server.wait_for_termination()


if __name__ == "__main__":
    print(f"Globals:\nMODEL={MODEL}\nBASE_URL={BASE_URL}\n"
          f"API_KEY={API_KEY}\nCLOUD_FOLDER={CLOUD_FOLDER}\n"
          f"SPEECH2TEXT_OPEN_AI={SPEECH2TEXT_OPEN_AI}\n"
          f"SPEECH2TEXT_MODEL={SPEECH2TEXT_MODEL}\n"
          f"BASE_URL_OPEN_AI={BASE_URL_OPEN_AI}\n"
          f"SECRET_KEY={'***' if SECRET_KEY else '(not set)'}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    if SECRET_KEY:
        print("\nüîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –í–ö–õ–Æ–ß–ï–ù–ê:")
        print("   - Ping: –ø—É–±–ª–∏—á–Ω—ã–π (–±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏)")
        print("   - NewMessage: —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é")
        print("   - AvailableModelsText2Text: —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é")
        print("   - AvailableModelsSpeech2Text: —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é")
    else:
        print("\n‚ö† –í–ù–ò–ú–ê–ù–ò–ï: SECRET_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞!")
    
    print("\nChecking available models...")
    check_arr = available_models(BASE_URL, API_KEY, CLOUD_FOLDER)
    check_arr_speech = available_models(BASE_URL_OPEN_AI,
                                        SPEECH2TEXT_OPEN_AI, None)
    if MODEL not in check_arr:
        print(f"ERROR: Text2Text model {MODEL} not found in available models!")
    elif SPEECH2TEXT_MODEL and (SPEECH2TEXT_MODEL not in check_arr_speech):
        print(f"ERROR: Speech2Text model {SPEECH2TEXT_MODEL} not found in "
              "available models!")
    else:
        print(f"Default Text2Text model set to {MODEL}\n"
              f"Default Speech2Text model set to {SPEECH2TEXT_MODEL}\n"
              "Starting server...")
        logging.basicConfig()
        serve()
